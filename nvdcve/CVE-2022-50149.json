{
  "cve": {
    "data_type": "CVE",
    "data_format": "MITRE",
    "data_version": "4.0",
    "CVE_data_meta": {
      "ID": "CVE-2022-50149",
      "ASSIGNER": "cve@kernel.org"
    },
    "problemtype": {
      "problemtype_data": [
        {
          "description": []
        }
      ]
    },
    "references": {
      "reference_data": [
        {
          "url": "https://git.kernel.org/stable/c/37f908038402c9b8325763f306a1c65d88757e15",
          "name": "https://git.kernel.org/stable/c/37f908038402c9b8325763f306a1c65d88757e15",
          "refsource": "",
          "tags": []
        },
        {
          "url": "https://git.kernel.org/stable/c/70fe758352cafdee72a7b13bf9db065f9613ced8",
          "name": "https://git.kernel.org/stable/c/70fe758352cafdee72a7b13bf9db065f9613ced8",
          "refsource": "",
          "tags": []
        },
        {
          "url": "https://git.kernel.org/stable/c/733ab0c19bf17f6ad7c2b580ede006e369d5ab1b",
          "name": "https://git.kernel.org/stable/c/733ab0c19bf17f6ad7c2b580ede006e369d5ab1b",
          "refsource": "",
          "tags": []
        },
        {
          "url": "https://git.kernel.org/stable/c/779b634714c51d05baaeff4868ce2fd9fc7399bf",
          "name": "https://git.kernel.org/stable/c/779b634714c51d05baaeff4868ce2fd9fc7399bf",
          "refsource": "",
          "tags": []
        },
        {
          "url": "https://git.kernel.org/stable/c/8191b6cd9ada09b675f17446d5872eb1f77685cb",
          "name": "https://git.kernel.org/stable/c/8191b6cd9ada09b675f17446d5872eb1f77685cb",
          "refsource": "",
          "tags": []
        },
        {
          "url": "https://git.kernel.org/stable/c/a93f33aeef4e6a94ae9c9d3f5b2f9085ad0572ec",
          "name": "https://git.kernel.org/stable/c/a93f33aeef4e6a94ae9c9d3f5b2f9085ad0572ec",
          "refsource": "",
          "tags": []
        }
      ]
    },
    "description": {
      "description_data": [
        {
          "lang": "en",
          "value": "In the Linux kernel, the following vulnerability has been resolved:\n\ndriver core: fix potential deadlock in __driver_attach\n\nIn __driver_attach function, There are also AA deadlock problem,\nlike the commit b232b02bf3c2 (\"driver core: fix deadlock in\n__device_attach\").\n\nstack like commit b232b02bf3c2 (\"driver core: fix deadlock in\n__device_attach\").\nlist below:\n    In __driver_attach function, The lock holding logic is as follows:\n    ...\n    __driver_attach\n    if (driver_allows_async_probing(drv))\n      device_lock(dev)      // get lock dev\n        async_schedule_dev(__driver_attach_async_helper, dev); // func\n          async_schedule_node\n            async_schedule_node_domain(func)\n              entry = kzalloc(sizeof(struct async_entry), GFP_ATOMIC);\n              /* when fail or work limit, sync to execute func, but\n                 __driver_attach_async_helper will get lock dev as\n                 will, which will lead to A-A deadlock.  */\n              if (!entry || atomic_read(&entry_count) > MAX_WORK) {\n                func;\n              else\n                queue_work_node(node, system_unbound_wq, &entry->work)\n      device_unlock(dev)\n\n    As above show, when it is allowed to do async probes, because of\n    out of memory or work limit, async work is not be allowed, to do\n    sync execute instead. it will lead to A-A deadlock because of\n    __driver_attach_async_helper getting lock dev.\n\nReproduce:\nand it can be reproduce by make the condition\n(if (!entry || atomic_read(&entry_count) > MAX_WORK)) untenable, like\nbelow:\n\n[  370.785650] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables\nthis message.\n[  370.787154] task:swapper/0       state:D stack:    0 pid:    1 ppid:\n0 flags:0x00004000\n[  370.788865] Call Trace:\n[  370.789374]  <TASK>\n[  370.789841]  __schedule+0x482/0x1050\n[  370.790613]  schedule+0x92/0x1a0\n[  370.791290]  schedule_preempt_disabled+0x2c/0x50\n[  370.792256]  __mutex_lock.isra.0+0x757/0xec0\n[  370.793158]  __mutex_lock_slowpath+0x1f/0x30\n[  370.794079]  mutex_lock+0x50/0x60\n[  370.794795]  __device_driver_lock+0x2f/0x70\n[  370.795677]  ? driver_probe_device+0xd0/0xd0\n[  370.796576]  __driver_attach_async_helper+0x1d/0xd0\n[  370.797318]  ? driver_probe_device+0xd0/0xd0\n[  370.797957]  async_schedule_node_domain+0xa5/0xc0\n[  370.798652]  async_schedule_node+0x19/0x30\n[  370.799243]  __driver_attach+0x246/0x290\n[  370.799828]  ? driver_allows_async_probing+0xa0/0xa0\n[  370.800548]  bus_for_each_dev+0x9d/0x130\n[  370.801132]  driver_attach+0x22/0x30\n[  370.801666]  bus_add_driver+0x290/0x340\n[  370.802246]  driver_register+0x88/0x140\n[  370.802817]  ? virtio_scsi_init+0x116/0x116\n[  370.803425]  scsi_register_driver+0x1a/0x30\n[  370.804057]  init_sd+0x184/0x226\n[  370.804533]  do_one_initcall+0x71/0x3a0\n[  370.805107]  kernel_init_freeable+0x39a/0x43a\n[  370.805759]  ? rest_init+0x150/0x150\n[  370.806283]  kernel_init+0x26/0x230\n[  370.806799]  ret_from_fork+0x1f/0x30\n\nTo fix the deadlock, move the async_schedule_dev outside device_lock,\nas we can see, in async_schedule_node_domain, the parameter of\nqueue_work_node is system_unbound_wq, so it can accept concurrent\noperations. which will also not change the code logic, and will\nnot lead to deadlock."
        }
      ]
    }
  },
  "configurations": {
    "CVE_data_version": "4.0",
    "nodes": []
  },
  "impact": {},
  "publishedDate": "2025-06-18T11:15Z",
  "lastModifiedDate": "2025-06-18T13:47Z"
}