{
  "cve": {
    "data_type": "CVE",
    "data_format": "MITRE",
    "data_version": "4.0",
    "CVE_data_meta": {
      "ID": "CVE-2023-29374",
      "ASSIGNER": "cve@mitre.org"
    },
    "problemtype": {
      "problemtype_data": [
        {
          "description": [

          ]
        }
      ]
    },
    "references": {
      "reference_data": [
        {
          "url": "https://twitter.com/rharang/status/1641899743608463365/photo/1",
          "name": "https://twitter.com/rharang/status/1641899743608463365/photo/1",
          "refsource": "MISC",
          "tags": [

          ]
        },
        {
          "url": "https://github.com/hwchase17/langchain/pull/1119",
          "name": "https://github.com/hwchase17/langchain/pull/1119",
          "refsource": "MISC",
          "tags": [

          ]
        },
        {
          "url": "https://github.com/hwchase17/langchain/issues/814",
          "name": "https://github.com/hwchase17/langchain/issues/814",
          "refsource": "MISC",
          "tags": [

          ]
        },
        {
          "url": "https://github.com/hwchase17/langchain/issues/1026",
          "name": "https://github.com/hwchase17/langchain/issues/1026",
          "refsource": "MISC",
          "tags": [

          ]
        }
      ]
    },
    "description": {
      "description_data": [
        {
          "lang": "en",
          "value": "In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method."
        }
      ]
    }
  },
  "configurations": {
    "CVE_data_version": "4.0",
    "nodes": [

    ]
  },
  "impact": {
  },
  "publishedDate": "2023-04-05T02:15Z",
  "lastModifiedDate": "2023-04-05T13:02Z"
}